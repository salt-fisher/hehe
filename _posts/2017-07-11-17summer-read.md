---
layout: post
title: 17年暑假读论文
---
## Saliency Driven Image Manipulation
### 动机
拍了一张照片，发现背景里的distractor比要拍的东西更显眼。一张运动比赛的照片，最喜欢的运动员在在人群里看不出来。

解决：提出一个方法来操纵图片中的物体的显著性。

<img src="/reading_list/a_0813_2.png" style="width: 600px;"/>



## Visual Saliency Based on Multiscale Deep Features
### 动机:
* 在原理上,用深度神经⺴络特别是 CNN 来建立视觉显著性 的计算模型很适合。
	- 深度人工神经⺴络和人类执行视觉注意力过程的视觉和识别 系统的结构相似。
	- CNN的卷积层的输出和简单细胞响应类似,而全连接层则模 拟了人识别系统的推理和决定。
* 虽然 CNN 特征被成功用在很多视觉识别任务上,但还没有 被用在显著性检测上。因为显著性检测要求⺴络学习到图片 区域和周围区域的对比,而不是识别区域内部的内容
<!--break-->
* 大部分识别任务:关注区域内部。输出主要由区域内部的内 容决定。
* 显著性检测:关注区域内部和外部。输出由区域和其周围区 域以及图片其 部分的对比度决定。

### 做法:
将区域和其周围区域以及图片其 部分的 CNN 特征串联 起来,共同作为这个区域的特征输入全连接神经⺴络,输出这个 区域的显著性。通过在训练集上最小化平方误差训练这个⺴络。

<img src="/reading_list/a1.png" style="width: 600px;"/>

### 完整算法
* 对图片进行从从粗到细的M级超像素分割。组成M个超像素集合$S = \{S_1, ..., S_M\}$
* 对于每级分割，用上述网络计算每个超像素$i$的初始显著性$a_i^I$。通过最小化下式使得显著图在空间上更连贯。其中$w_{ij}$和超像素$i, j$之间的边缘强度有关。
$$\sum_i(a^R_i - a_i^I) + \sum_{i, j}w_{ij} (a^R_i - a^R_j)^2$$
* 对$M$级分割分别进行上述操作，得到$M$个显著图$A^{(1)}, ..., A^{(M)}$。最终的显著图是它们的线性组合$A = \alpha_k A^{(k)}$。系数$\alpha_k$通过在验证集上最小化平方误差学习得到。

## Saliency Detection with Recurrent Fully Convolutional Networks
### 动机
* 手工特征和经验先验存在局限性
* 现有的基于CNN的方法的问题：
	* 一些经验先验在显著性检测中确实有效，但大部分基于CNN的方法完全忽略了这些先验
	* 前作通常以图像块为处理单元，分别计算每个图像块的标签。产生的显著性标签图在空间上不够连贯。
	* 前作通常将显著性检测作为一个二分类问题。而只用两个标签来监督训练有那么多参数的网络，标签所能提供的监督太弱了。

### 做法
* 循环全卷积网络：
	* 每次迭代，将RGB图像和显著性图先验串联，输入FCN。输出的显著图作为下一次迭代时输入的先验图。
	* 初始的先验图通过结合一系列经验先验得到。
* 利用语意分割数据集进行预训练。语意分割数据集中的真值为21个类别的分割结果，加强了标签所能提供的监督。
<img src="/reading_list/rfcn.png" style="width: 600px;"/>

### 细节
* 卷积$F(\cdot; \theta)$＋解卷积$U(\cdot; \psi)$结构的RFCN：
	* 第1次迭代，输入RGB图像$I$和显著性先验图$P$，输出显著图和背景图$Y^1 = U(F(I; \theta), P; \psi)$
	* 第t次迭代，输入RGB图像$I$和上一次迭代时输出$Y^{t-1}$中的显著图$H^{t-1}$，输出显著图和背景图$Y^t = U(F(I; \theta), H^{t-1}; \psi)$
* 在含有C+1个类的语意分割数据集上预训练
	* RFCN的输入仍是4通道（RGB＋显著性先验图），输出C+3个通道，即C+1个语意类、显著图、背景图
	* 将所有语意类的并集作为显著图真值，不属于任何语意类的像素作为背景。－－－作为预训练集的显著性检测真值
	* 训练时，误差为 语意分割误差 和 显著性检测分类误差 相加。
* 在显著性检测数据集上训练
	* 将预训练后的RFCN输出滤波器中，输出语意分割结果的滤波器去掉。改成输出2通道：显著图、背景图
	* 用同样的方法，在显著性检测数据集上训练。

## Saliency Detection by Multi-Context Deep Learning
### 动机:
* 利用传统的底层特征不能有效地从背景环境中区分高级概念上显著的物体。
* 合适的预训练能让深度模型有识别物体的能力，提供高级的语意信息。
* 但这些信息只是一般性地识别图片中的物体，并不能指出其中哪一个是显著物体。要从多个物体中识别显著的物体，还考虑全局环境。

<img src="/reading_list/m1.png" style="width: 600px;"/>

### 做法：
* 同时考虑局部环境和全局环境的深度学习模型。
* task-specific的预训练方法。

全局环境：包含整个图片。由原图补0，平移到以考虑的超像素为中心的位置。输出一个全局显著图。

局部环境：包含以考虑的超像素为中心的小窗口。输出2096维特征。

综合：输入4096维局部环境特征，串联全局模型输出的全局显著图。

<img src="/reading_list/a2.png" style="width: 600px;"/>

### task-specific的预训练方法
* 图片分类预训练（图像级别的标签）
* 目标定位预训练（目标级别标签）
* 本文提出的产生超像素级别标签进行预训练的方法：从目标框中随机抽取超像素，赋予这个框的标签。

## Deep Networks for Saliency Detection via Local Estimation and Global Search
### 动机：
* 一些模型基于局部对比度，缺乏全局信息，往往高亮了物体的边缘。
* 一些模型用某图片元素在图片全局环境中的“稀缺性”和“独特性”来衡量它的显著性。但当显著物体的外观和背景相似时失效。
* 现有的结合全局模型和局部模型的方法利用手工特征和经验先验。可能失效。
* 取得了巨大成功的深度神经网络还没被用在显著性上。这是因为当时用CNN的用法都是将一个图像块输入CNN，然后得到这个图像块的类别作为输出。但显著性却需要更多地考虑图像区域之间的关系，而不是图像块本身。

### 做法：
* 局部模型DNN-L：
	- 将\textbf{以某像素为中心的51*51矩形区域}输入CNN，输出\textbf{这个像素}显著／不显著的标签。
	- 用\textbf{矩形区域级别的真值}训练。
* 全局模型DNN-G：
	- 计算单元为GOP proposal。
	- 根据DNN-L输出的初始显著图求每个proposal的A（proposal像素和初始图显著像素的交集／proposal像素总量）和C（proposal像素和初始图显著像素的交集／初始图显著像素总量）
	- 将每个GOP的上述A、C以及一些全局特征输入DNN-G，输出每个GOP和真值图显著像素的精确度和重叠率。
	- 用\textbf{GOP级别的真值}精确度、重叠率训练。

<img src="/reading_list/a3.png" style="width: 600px;"/>

## DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection}
### 动机：
* 同上
* 之前三个用CNN做显著性的方法都需要先分割成超像素或GOP，耗费时间。

### 做法：
* GV-CNN：输入图片，输出低分辨率（28*28）显著图。用真值训练。
* HRCNN：从低分辨率得到高分辨率。
	- 从训练好的CG-CNN开始，每一个低分辨率显著图最近邻插值放大RCL，输出稍大的显著图。直到和原图一样大。
	- 从小到大的每个图都用真值监督训练。
	- RCL：循环卷积层。t步地输出为输入经过卷积＋t-1经过卷积

<img src="/reading_list/a4.png" style="width: 600px;"/>

## Deep Contrast Learning for Salient Object Detection
### 动机
* 基于手工特征的方法的局限性
* 已有的用深度学习的显著性检测方法以图像块作为处理单元
	- 给每个像素的显著值其实是包含它的一个图像块显著值，造成模糊
	- 每运行一次CNN，得到的是一个图像块的标签。需要运行很多次CNN才能得到整张图片的。计算量大

### 做法
* MS-FCN: 全卷积网络。对整张图片只需运行一次这个网络就能得到像素级别的显著性图。
* 分割级的池化法（类似于ROI pooling）。可以用全连接网络计算每个超像素的标签。

<img src="/reading_list/a5.png" style="width: 600px;"/>

### 实现细节
MS-FCN:
* 将VGG的两个全连接层换成滤波器大小为1*1的卷积层。输出大小为原图的1/8的显著图1
* 原本VGG的5个卷积层输出的尺寸分别是原图的 1/2, 1/4, 1/8, 1/16, 1/32。为了让输出的预测图不要太小，将最后两个卷积层的下采样取消。
* 为了保持最后4层的接受域大小不变，在conv4, conv5用stride=2的扩张卷积；在conv6, conv7用stride=4的扩张卷积。

分割级的池化法，用全连接网络计算每个超像素的标签
* 把每个超像素的mask对应到conv5输出的特征图像
	- conv5特征图的每个像素的接受域都对应输入图片中的8*8区域，因此可能被多个超像素包含
* 将每个超像素的conv5特征、它的直接邻居的特征、整张图片除开它之外所有超像素的特征串联，输入全连接层，得到这一超像素的显著性标签。
* 对每个超像素进行上述操作。得到显著图2

* 显著图1和2串联成两个通道，再经过一个卷积层，得到输出的显著图3

训练：每个epoch固定两个分支中的一个，更新另一个，交替训练。

用CRF改善图3：最小化
$$E(L) = -\sum_i \log P(l_i) + \sum_{ij} \theta_{ij}(l_i, l_j)$$
$P(l_i)$为像素i被赋予正标签的概率（取显著图3），$\theta_{ij}(l_i, l_j)$表示“i被赋予正标签”与“j被赋予正标签”的可能性。

### 其他
除了和作者的前作相同的方面（输入超像素、其邻居、整图片出了它自己），对比度还有体现在哪。
滤波器大小为1*1的卷积层用扩张卷积哪里扩大接受域

## Saliency Unified: A Deep Architecture for simultaneous Eye Fixation Prediction and Salient Object Segmentation
### 动机
* 眼睛注视点预测和显著目标分割很相关

### 做法
* 和前一篇一样的处理，增大输出尺寸到1/8原图并用扩张卷积保持接受域大小不变。
* 用inception layer引入多尺度：由多个大小、stride、扩张等不同的滤波器组成
* 和前一篇一样用了CRF改善
* 这两个任务互有帮助

<img src="/reading_list/a6.png" style="width: 600px;"/>

## Recurrent Attentional Networks for Saliency Detection
### 动机
* 已有的基于CNN的显著性检测方法都是处理图像块
* 一般的CNN结构会下采样输入图片，即输出的尺寸比输入小，不适合显著性这种需要输出稠密标签的任务

### 做法
* 用卷积-解卷积结构输出尺寸和原图一样大的标签图。
* 用循环注意力机制改善标签图的细节。

<img src="/reading_list/a7_1.png" style="width: 600px;"/>
<img src="/reading_list/a7_2.png" style="width: 600px;"/>

### 细节
* 从输入图片$x$得到显著图$r$: $z = \mbox{CNN}(x), r = \mbox{DecCNN}(z)$
* 仿射变换可以由变换矩阵$\tau$决定。设一个点变换前的坐标为$\textbf{x}$，变换后的坐标为$\textbf{y}$，则有$(\textbf{y}^T, 1)^T = \tau (\textbf{x}^T, 1)^T$。本文只考虑平移和放大的尺度变换。那么就是原图经过由$\tau$决定的变换，得到的是原图中的一个图像块，并放大到和原图一样大。
* 从$i=1$开始，$\tau_0$对原图不做变化，$h_0^1, h_0^2$只由原图做卷积得到，$r_0$为输入图片经（另外一组）卷积解卷积结构得到的显著图。
	- $x_i = \mbox{ST}(x, \tau_i)$, 
	- $z_i = \mbox{CNN}_r(x_0)$, 
	- $h_i^1 = \phi(W_I^1 * z_i + W_R^1 * h_{i-1}^1 + b^1)$
	- 更新显著图：$r_i = r_{i-1} + \mbox{ST}(\mbox{DecCNN}_r(h_i), \tau_i^{-1}$
	- 计算下一次循环关注的区域，即能产生相应区域的变换矩阵：
		- $h_i^2 = \phi(W_I^2 * h_i^1 + W_R^2 * h_{i-1}^2 + b^2)$
		- $\tau_{i+1} = \phi(W_{loc2} * \phi(W_{loc1}*h_i^2))$

## Deep Saliency with Encoded Low level Distance Map and High Level Features
### 动机
* CNN特征图尺寸太小，只用CNN特征产生模糊的结果
* 其他利用CNN的方法，大部分都计算每个图像块（比如超像素）的显著值，但是不能有效区分显著区域和背景，因为它们没有直接编码图像块之间的距离。

### 做法
* 对于每张图片，用VGG得到深度特征。
* 图片分割成超像素，和23*23的格子。求每个超像素的ELD map
	* 每个超像素和一个与它重叠最大的格子对应。
	* 对于每个超像素，计算它和每个格子在多个手工特征空间的距离，得到46个通道的23*23的距离图。
	* 这个距离图与格子的颜色特征串联，得到54个通道的23*23距离图。
	* 上述距离图与VGG深度特征串联。经过全连接得到这个超像素的显著值。

<img src="/reading_list/a8.png" style="width: 600px;"/>
