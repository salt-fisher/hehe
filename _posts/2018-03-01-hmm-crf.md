---
layout: post
title: HMM, CRF
---
[这位雷锋的笔记HMMs](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf)

[还有CRF](http://www.cs.columbia.edu/~mcollins/crf.pdf)

[CRF求参数用到的后向前向法](http://www.cs.columbia.edu/~mcollins/fb.pdf)

## Trigram HMMs
输入序列，输出标签序列。例如，输入一句话the dog saw cat laughs，输出这句话里每个词语的类型D N V N V。

设输入序列中的所有可能元素集合是$\mathcal{V}$，输出序列的是$\mathcal{K}$，所有输入/输出序列对$x_1 ... x_n, y_1 ... y_n$组成的集合是$\mathcal{S}$

目标是求从输入序列$x$到（预测）输出序列的映射$f(x)$。为了实现这一目标，首先分别求$P(x \| y)$和$P(y)$。然后对于输入序列$x$，把使得$P(x, y)$最大的$y$作为预测结果：$f(x) = \arg \max_{y\in \mathcal{Y}} P(y) P(x \| y)$。
在训练样本里有些可能的序列没有出现或者只出现了很少次，直接用频率估计概率不会准确。但是短的片段倒是重复出现了很多次，所以估计短的片段的概率比较准确。为了求$P(x \| y)$和$P(y)$，作出了两个独立假设，得以用短的片段的概率估计整个序列的概率；<!--break-->

有了$P(x, y)$，使其最大的$y$可以通过遍历所有可能的$y$的得到，但是这样的计算量太大几乎做不了。所以用维特比解码法求出使$P(x, y)$最大的$y$。

### 两个假设
假设

$$P(x_1, x_2, .., x_n | y_1, .., y_{n+1}) = \prod_{i=1}^n P(x_i|y_i)$$

$$P(y_1, ..., y_{n+1}) = \prod_{i=1}^{n+1} P(y_i|y_{i-1}, y_{i-2})$$

而$P(y_i\|y_{i-1}, y_{i-2})$和$P(x_i\| y_i)$都能比较准确地从训练集里估计出来（统计这些样本在总体里出现的频率）。于是根据$P(x_1, ..., x_n, y_1, ..., y_{n+1}) = P(x_1, ..., x_n\|y_1, ..., y_{n+1}) P(y_1, ..., y_{n+1})$就能求出$P(x, y)$了：

$$P(x, y) = \prod_{i=1}^n P(y_i|y_{i-1}, y_{i-2}) \prod_{i=1}^n P(x_i|y_i)$$

### Viterbi算法
为了我的愚蠢大脑，只好写得啰嗦点。给定$x_1, ..., x_n$，已知$P(x_1, ..., x_n, y_1, ..., y_{n+1})$，怎样求最可能的标签序列$\arg \max_{y_1, ..., y_{n+1}} P(x_1, ..., x_n, y_1, ..., y_{n+1})$？

把以$y_{n-1}, y_{n}$结尾的、长为n的序列$y_1, ..., y_{n-1}, y_n$和输入序列的联合概率最大值记为$\pi(n, y_{n-1}, y_n)$（这是个关于$y_{n-1}, y_n$的函数，因为$y_1, ..., y_{n-2}$都被取最大值给消掉了）。则有：

$$\pi(n, y_{n-1}, y_n) = \max_{y_1, ..., y_{n-2}} P(x_1, ..., x_k, y_1, ..., y_{n-2}, y_{n-1}, y_n) $$

$$= \max_{y_1, ..., y_{n-2}} P(x_1, ..., x_{n-1}, y_1, ..., y_{n-3}, y_{n-2}, y_{n-1}) P(y_n| y_{n-2}, y_{n-1}) P(x_n|y_n)$$

$$=\max_{y_{n-2}} \max_{y_1, ..., y_{n-3}}P(y_1, ..., y_{n-3}, y_{n-2}, y_{n-1}) P(y_n|y_{n-2}, y_{n-1}) P(x_n|y_n)$$

$$= \max_{y_{n-2}} \pi(n-1, y_{n-2}, y_{n-1}) P(y_n|y_{n-2}, y_{n-1}) P(x_n|y_n)$$

这就得到了$\pi(n, y_{n-1}, y_n)$的递推关系了。

而又有

$$\max_{y_1, ..., y_{n}} P(x_1, ..., x_n, y_1, ..., STOP) = \max_{y_{n-1}, y_n} \pi(n, y_{n-1}, y_n) P(STOP|y_{n-1}, y_n)$$

因此，使得$P(x_1, ..., x_n, y_1, ..., STOP)$取最大值的$y_1, ..., y_n$里面的$y_{n-1}, y_n$就是使得$\pi(n, y_{n-1}, y_n) P(STOP\|y_{n-1}, y_n)$取最大值的$y_{n-1}, y_n$。

为了求$\arg\max_{y_1, ..., y_n} P(x_1, ..., x_n, y_1, ..., y_n, STOP)$，得先求一个关于$y_{n-1}, y_n$的函数$\pi(n, y_{n-1}, y_n)P(STOP\|y_{n-1}, y_n)$，把它取argmax可以得到$y_{n-1}, y_n$。

为了求这$\pi(n, y_{n-1}, y_n)$，得先求关于$y_{n-2}, y_{n-1}, y_{n}$的函数$\pi(n-1, y_{n-2}, y_{n-1}) P(y_n\|y_{n-1}, y_{n-2}) P(x_n\|y_n)$并对$y_{n-2}$求最大值消去$y_{n-2}$（当然还要保存产生这个最大值的$y_{n-2}$）。

为了求$\pi(n-1, ...)$，得先求关于$y_{n-3}, y_{n-2}, y_{n-1}$的函数$\pi(n-2, y_{n-3}, y_{n-2}) P(y_{n-1}\|y_{n-2}, y_{n-3}) P(y_{n-1}\|x_{n-1})$并对$y_{n-3}$取最大值消去$y_{n-3}$（当然还要保存产生这个最大值的$y_{n-3}$）。

。。。

为了求$\pi(n-k, ...)$，得先求关于$y_{n-k-2}, y_{n-k-1}, y_{n-k}$的函数$\pi(n-k-1, y_{n-k-2}, y_{n-k-1}) P(y_{n-k}\|y_{n-k-1}, y_{n-k-2}) P(y_{n-k}\|x_{n-k})$并对$y_{n-k-2}$取最大值消去$y_{n-k-2}$（当然还要保存产生这个最大值的$y_{n-k-2}$）。

。。。

=================分割线====================

最终k增大到k=n-3了，终于到达了递归的终点该返回了

为了求$\pi(3, y_2, y_3)$，得先求关于$y_1, y_2, y_3$的函数$\pi(2, y_1, y_2) P(y_3\|y_2, y_1) P(y_3\|x_3)$并对$y_1$取最大值消去$y_1$（当然还要保存产生这个最大值的$y_1$）。这就是说，对于每一组可能的$y_2, y_3$，都保存一个使得函数$\pi(2, y_1, y_2) P(y_3\|y_2, y_1) P(y_3\|x_3)$最大的$y_1$以及相应的函数值。比如说可以用两个二维数组存放这两个东西。第一个数组是各个$y_2, y_3$对应的$\pi(3, y_2, y_3)$，第二个数组是产生最大值的$y_1$。

得到了$\pi(3, y_2, y_3)$，接下来求$\pi(3, y_2, y_3) P(y_4\| y_3, y_2) P(y_4\|x_4)$并对$y_2$取最大值消去$y_2$（当然还要保存产生这个最大值的$y_2$）。同样产生了两个二维数组，第一个数组是各个$y_3, y_4$对应的$\pi(4, y_3, y_4)$，第二个数组是产生最大值的$y_2$。

。。。

现在又回到了$\arg \max_{y_{n-1}, y_n} \pi(n, y_{n-1}, y_n)P(STOP\|y_{n-1}, y_n)$。这一路上，已经产生了一系列成对的二维数组。其中最后一对是：(1) 各个$y_{n-1}, y_n$对应的$\pi(n, y_{n-1}, y_n)$和 (2) 产生此最大值的$y_{n-2}$。利用(1)可以得出各个$y_{n-1}, y_n$对应的$\pi(n, y_{n-1}, y_n)P(STOP\|y_{n-1}, y_n)$，找出使其最大的$y_{n-1}, y_n$，在利用(2)找到$y_{n-2}$，依次循着来时的足记，直到使用第一次产生的二维数组找到$y_1$。。。。好复杂

分割线以上的部分是脑子里的内容。实际实现的时候，从分割线开始就行了。

## CRF
输入序列$x_1, ..., x_n$，标签序列$y_1, ..., y_n$。给定从序列x和y到d维向量的特征函数

$$\phi(x, y) = \sum_{i=1}^n \phi(x, i, y_{i-1}, y_i)$$

CRF模型把以输入序列为条件的标签序列的分布定义成这样的参数函数：

$$P(y|x; w) = \frac{\exp (w \cdot \phi(x, y))}{\sum_{y' \in \mathcal{K}^n} \exp (w \cdot \phi(x, y'))}$$

其中参数w通过在训练集上最大化对数似然函数得到，此步骤需要用到后向前向法求损失函数关于w的梯度。用CRF预测标签序列需要用到类似于HMM的维特比解码法。

## 用于CRF的维特比算法
对于输入序列$x$，怎样求使得$P(y\|x)$最大的标签序列$y$？首先显然，使得$P(y\|x)$最大的$y$也就是使得$w \cdot \phi(x, y)$最大的$y$。即求：

$$\arg \max_{y_1, ..., y_n} \sum_{i=1}^n w \cdot \phi(x, i, y_{i-1}, y_i) $$

把关于$y_n$的函数$\max_{y_1, ..., y_{n-1}} \sum_{i=1}^n w \cdot \phi(x, i, y_{i-1}, y_i)$记为$\pi(n, y_n)$，则有

$$\pi(n, y_n) = \max_{y_1, ..., y_{n-1}} \sum_{i=1}^n w \cdot \phi(x, i, y_{i-1}, y_i)$$

$$=\max_{y_{n-1}}\max_{y_1, ..., y_{n-2}} \sum_{i=1}^{n-1} w \cdot \phi(x, i, y_{i-1}, y_i) + w \cdot \phi(x, y_{n-1}, y_n)$$

$$=\max_{y_{n-1}}[\pi(n-1, y_{n-1}) + w \cdot \phi(x, y_{n-1}, y_n)]$$

由此可得一个和HMM的维特比解码法差不多的方法，脑中内容和HMM类似，实现上是这样的：

首先求$\pi(2, y_2) = \max_{y_1} w \cdot \phi(x, 1, *, y_1) + w \cdot \phi(x, 2, y_1, y_2)$，这是一个关于$y_2$的函数。这就是说，对于$y_2$的每一个取值，都有一个使$w \cdot ...$最大的$y_1$。把它保存在一个一维数组中。对应的函数值$\pi(2, y_2)$也用一个一维数组保存。保存函数值的那个数组用于下一步计算，保存$y_1$的那个数组用于将来寻找来时的路。

然后求$\pi(3, y_3) = \max_{y_2} \pi(2, y_2) + w \cdot \phi(x, y_2, y_3)$。这是一个关于$y_3$的函数。对于$y_3$的每一个取值，都有一个使得$\pi(2, y_2) + w \cdot \phi(x, y_2, y_3)$最大的$y_2$。把$y_3$的每个取值对应的$y_2$和那个最大值分别保存在两个一维数组中。

。。。

终于到达了终点，求出了关于$y_n$的函数$\pi(n, y_n)$了。这一步同样产生了两个一维数组，一个是每个$y_n$所对应的函数值$\pi(n, y_n)$，另一个是对应的$y_{n-1}$。取第一个一维数组的最大值，这就是我们要求的$y_n$。用第二个数组找到对应的$y_{n-1}$。依次类推，利用之前的每一步所产生的一维数组，用$y_{n-1}$找$y_{n-2}$，用$y_{n-2}$找$y_{n-3}$。。。直到得到整个序列。

## 估计参数$w$, forward-backward算法
在m个训练样本$(x^j, y^j), j=1, ..., m$上最大化regularized对数似然函数：

$$L(w) = \sum_{j=1}^m \log P(y^j | x^j; w) - \frac{\lambda}{2} \| w \|^2$$

上式对$w$求梯度得：

$$\nabla_w L(w) = \sum_{j=1}^m ( \phi(x^j, y^j) - \sum_{y' \in \mathcal{K}^n} P(y' | x^j; w) \phi(x^j, y') - \lambda w )$$

其中第一项和第三项都容易计算，而第二项需要对所有可能的标签序列（共有$\|\|\mathcal{K}\|\|^n$个）求和，直接算的话计算量太大。因此，要用前向后向法计算$\sum_{y \in \mathcal{K}^n} P(y \| x; w) \phi(x, y)$。

$$\sum_{y\in \mathcal{K}^n} P(y | x; w) \sum_{i=1}^n \phi(x, i, y_{i-1}, y_i)$$

$$=\sum_{i=1}^n \sum_{y\in \mathcal{K}^n} P(y | x; w) \phi(x, i, y_{i-1}, y_i)$$

$$=\sum_{i=1}^n \sum_{y_{i-1}, y_i \in \mathcal{K}^2} \phi(x, i, y_{i-1}, y_i) \sum_{y\in \mathcal{K}^{n-2}}P(y|x;w)$$

计算量大的部分在于$\sum_{y_1 \in \mathcal{K}} ... \sum_{y_{i-2} \in \mathcal{K}} \sum_{y_{i+1}} ... \sum_{y_n \in \mathcal{K}} P(y\|x;w)$。这是一个关于$y_{i-1}, y_i$的函数（当然它也和x有关，在这里先省略），通过对$y$中除了它俩之外的所有元素求和得到。把这个函数记为$q(i, y_{i-1}, y_i\|x)$，意思是，把$P(y\|x;w)$对序列$y$除了第i和第i-1个元素之外的所有元素进行求和。

$$q(k, y_{k-1}, y_k|x) = \sum_{y_1} ... \sum_{y_{k-2}}\sum_{y_{k+1}} ... \sum_{y_n} \frac{\exp( \sum_{i=1}^n w \cdot \phi(x, i, y_{i-1}, y_i) )}{\sum_{y'\in \mathcal{K}^n} \exp(  w \cdot \phi(x, y') ) }$$

$$=\sum_{y_1} ... \sum_{y_{k-2}}\sum_{y_{k+1}} ... \sum_{y_n} \frac{ \prod_{i=1}^n \exp( w \cdot \phi(x, i, y_{i-1}, y_i) )}{\sum_{y'\in \mathcal{K}^n} \exp(  w \cdot \phi(x, y') ) }$$

将$\sum_{y'\in \mathcal{K}^n} \exp(w \cdot \phi(x, y'))$记为$Z$，将$\exp(w \cdot \phi(x, i, y_{i-1}, y_i))$记为$\psi(x, i, y_{i-1}, y_i)$，上式就可以写成

$$q(k, y_{k-1}, y_k | x) =\frac{1}{Z} \sum_{y_1} ... \sum_{y_{k-2}}\sum_{y_{k+1}} ... \sum_{y_n} \psi(x, 1, *, y_1) \psi(x, 2, y_1, y_2) ... \psi(x, n, y_{n-1}, y_n)$$

首先求$Z$。应用若干次乘法结合律可以得到：

$$Z=  \sum_{y_1} ... \sum_{y_n} \prod_{i=1}^n \psi(i, y_{i-1}, y_i)$$

$$=\sum_{y_1}\psi(1, *, y_1) \sum_{y_2}\psi(2, y_1, y_2) ... \sum_{y_n}\psi(n, y_{n-1}, y_n)$$

解释一下：

首先从$y_n, y_{n-1}$开始，对于$y_{n-1}$的每一个可能的取值，都计算出关于$y_{n-1}$的函数$\sum_{y_n}\psi(n, y_{n-1}, n)$的值。结果可以保存在一个长为$\|\|\mathcal{K}\|\|$的n号一维数组中。

然后，对于$y_{n-2}$的每一个可能的取值，都乘上刚才计算出来的n号一维数组并对n号一维数组求和，得到n-1号一维数组。

。。。

最后，对于$y_1$的每一个可能的取值，都乘上刚才计算出来的2号1维数组并对2号一维数组求和，得到1号一维数组。对这个1维数组再求和就得到了$Z$的值。

$q(k, y_{k-1}, y_k\|x)$的计算也是如法炮制，根据

$$q(k, y_{k-1}, y_k |x) = \frac{1}{Z} \psi(x, k, y_{k-1}, y_k) \sum_{y_1}\psi(x, 1, *, y_1) \sum_{y_2} \psi(x, 2, y_1, y_2) ...$$

$$\sum_{y_{k-2}}\psi(x, k-2, y_{k-3}, y_{k-2}) \sum_{y_{k+1}}\psi(x, k+1, y_k, y_{k+1}) ...\sum_{y_n}\psi(x, n, y_{n-1}, y_n)$$

对于$y_{k-1}, y_k$的所有可能取值，执行类似于求$Z$的那一套步骤。每套类似于求$Z$的步骤都需要进行$n-1$次乘法运算，而$y_{k-1}, y_k$的可能取值共有$\|\|\mathcal{K}\|\|^2$个，因此，计算$q(k, y_{k-1}, y_k\|x)$的时间复杂度是$O(n \|\|\mathcal{K}\|\|^2)$。
