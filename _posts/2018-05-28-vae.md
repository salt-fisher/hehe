---
layout: post
title: 变分自编码
---
## kkk
假设图片$X$是由隐变量$Z\sim p(\textbf{z})$生成的。其中$p(\textbf{z})$已知，而且$p(\textbf{x}|\textbf{z}; \theta) = \mathcal{N}(f(\textbf{z};\theta), \sigma^2 I)$。因此

$$p(\textbf{x};\theta) = \int p(\textbf{x}|\textbf{z};\theta) p(\textbf{z}) dz = \mathbb{E}_{Z\sim p(\textbf{z})} [p(\textbf{x}|\textbf{z};\theta)]$$

现在有一些样本$X_1, ..., X_n$，就可通过最大化似然函数$\prod_{i=1}^n p(\textbf{x}_i;\theta)$来求$\theta$。

一个办法是用均值近似数学期望$\mathbb{E}_{Z\sim p(\textbf{z})} [p(\textbf{x}_i|\textbf{z};\theta)]$，即采样M个$\textbf{z}_j$，然后求$\frac{1}{M} \sum_{j=1}^M p(\textbf{x}_i|\textbf{z}_j;\theta)$。这就是说，对于每个训练样本，都产生M个随机向量输入CNN产生M个生成图片、然后求它们和训练样本的均方误差的均值，以此作为损失函数更新CNN。

然而对于大部分的$\textbf{z}$，$p(\textbf{x}|\textbf{z})$都几乎是0。上述做法只有当M特别大的时候才准确。所以需要寻找其它办法求$\mathbb{E}_{Z\sim p(\textbf{z})} [p(\textbf{x}_i|\textbf{z};\theta)]$。

其它的办法是从更加可能产生图片的$Z \sim q(\textbf{z})$里取随机变量估计$\mathbb{E}_{Z\sim q(\textbf{z})} [p(\textbf{x}_i|\textbf{z};\theta)]$、并且利用$p(\textbf{x})$和$\mathbb{E}_{Z\sim q(\textbf{z})} [p(\textbf{x}_i|\textbf{z};\theta)]$的关系求$p(\textbf{x}; \theta)$

$$\log p(\textbf{x}) - \mathcal{D}[q(\textbf{z}|\textbf{x}) || p(\textbf{z}|\textbf{x})] = \mathbb{E}_{\textbf{z}\sim q}[\log p(\textbf{x}|\textbf{z})] - \mathcal{D}[q(\textbf{z}|\textbf{x}) || p(z)]$$

其中等号左边是对数似然的下界（因为KL距离大于0），这就是VAE要最大化的目标函数了。右边的数学期望就用随机取的一个$\textbf{z}$所算出的值近似。$q(\textbf{z}|\textbf{x})$用一个正态分布表示，其均值和方差都是关于$\textbf{x}$的函数。这样一来等号右边的两项就都可以计算了。

这就是说，对于每个训练样本、先输入编码器得到两个向量作为正态分布$q(\textbf{z}|\textbf{x})$的均值和方差、然后从这种正态分布里采一个样本$\textbf{z}$（实现上为了后向传递是从标准正态分布里踩个样本然后乘以标准差并加上均值）、然后输入解码器得到生成图片、生成图片和训练样本的均方误差用来计算数学期望那一项、编码器输出的均值和方差用来计算和标准正态分布的KL距离那一项。

